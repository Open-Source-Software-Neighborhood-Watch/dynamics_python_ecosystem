{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This file reads in the top libraries contributor networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import bokeh\n",
    "import glob\n",
    "import csv\n",
    "import math\n",
    "import powerlaw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update this cell with your working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put your path here for unzipped Python_Eco_GHT_data\n",
    "work_dir = r\"C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\**\\*.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You have to help clean the data. Go into each commits file and delete the column one past commit_comments (AG is opened in a spreadsheet). If you receive an error just go to the line number and delete the comments that wnet into a new cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Click\\click_commits.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Click\\click_project_members.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Click\\click_pull_requests.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Click\\click_pull_requests_actions.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\CPython\\cpython_commits.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\CPython\\cpython_project_members.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\CPython\\cpython_pull_requests.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\CPython\\cpython_pull_requests_actions.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Django\\django_commits.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Django\\django_project_members.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Django\\django_pull_requests.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Django\\django_pull_requests_actions.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Matplotlib\\matplotlib_commits.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Matplotlib\\matplotlib_project_members.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Matplotlib\\matplotlib_pull_requests.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Matplotlib\\matplotlib_pull_requests_actions.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Numpy\\numpy_commits.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Numpy\\numpy_project_members.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Numpy\\numpy_pull_requests.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Numpy\\numpy_pull_requests_actions.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Odoo\\odoo_commits.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Odoo\\odoo_project_members.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Odoo\\odoo_pull_requests.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Odoo\\odoo_pull_requests_actions.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Pandas\\pandas_commits.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Pandas\\pandas_project_members.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Pandas\\pandas_pull_requests.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Pandas\\pandas_pull_requests_actions.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Pytest\\pytest_commits.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Pytest\\pytest_project_members.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Pytest\\pytest_pull_requests.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Pytest\\pytest_pull_requests_actions.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Pytest-cov\\pytest-cov_commits.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Pytest-cov\\pytest-cov_project_members.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Pytest-cov\\pytest-cov_pull_requests.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Pytest-cov\\pytest-cov_pull_requests_actions.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Pyyaml\\pyyaml_commits.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Pyyaml\\pyyaml_project_members.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Pyyaml\\pyyaml_pull_requests.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Pyyaml\\pyyaml_pull_requests_actions.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Requests\\requests_commits.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Requests\\requests_project_members.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Requests\\requests_pull_requests.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Requests\\requests_pull_requests_actions.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Scipy\\scipy_commits.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Scipy\\scipy_project_members.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Scipy\\scipy_pull_requests.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Scipy\\scipy_pull_requests_actions.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Setuptools\\setuptools_commits.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Setuptools\\setuptools_project_members.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Setuptools\\setuptools_pull_requests.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Setuptools\\setuptools_pull_requests_actions.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Six\\six_commits.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Six\\six_project_members.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Six\\six_pull_requests.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Six\\six_pull_requests_actions.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Sphinx\\sphinx_commits.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Sphinx\\sphinx_project_members.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Sphinx\\sphinx_pull_requests.csv\n",
      "C:\\Users\\thoma\\PycharmProjects\\GHT_python_data\\Sphinx\\sphinx_pull_requests_actions.csv\n"
     ]
    }
   ],
   "source": [
    "#Dictionary below gives order of files for each sub_directory of files\n",
    "#working_list = {\"commits\":{}, \"members\":{}, \"requests\":{}, \"actions\":{}}\n",
    "#logins = []\n",
    "working = []\n",
    "#read in csv files, comments with quotations in them are problematic\n",
    "for filepath in glob.iglob(work_dir):\n",
    "    #Skip files without specific repo data\n",
    "    if \"Top_Contributor_Networks\" in filepath: \n",
    "        pass\n",
    "    elif \"Top_Contributers\" in filepath:\n",
    "        pass\n",
    "    elif \"repo_creation_dates\" in filepath: \n",
    "        pass\n",
    "    elif \"Python-dateutil\" in filepath: \n",
    "        pass #not one of top 14\n",
    "    else: \n",
    "        #Still had issues with quotation marks had to change about 7\n",
    "        print (filepath)\n",
    "        #Code that was later updated\n",
    "        #if \"logins\" in filepath: \n",
    "        #    logins.append(pd.read_csv(filepath,low_memory=False).replace('\"','', regex=True).replace(\"'\",'', regex=True).replace(\"`\",\"\", regex=True))\n",
    "        #else: \n",
    "        working.append(pd.read_csv(filepath,low_memory=False).replace('\"','', regex=True).replace(\"'\",'', regex=True).replace(\"`\",\"\", regex=True))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#breaks into chunks for library files \n",
    "working_chuncked = []\n",
    "count = 0\n",
    "inter = []\n",
    "for idx in range(len(working)):\n",
    "    #print (idx+1, (idx+1)%4)\n",
    "    if (idx+1)%4 == 0: \n",
    "        \n",
    "        inter.append(working[idx])\n",
    "        working_chuncked.append(inter)\n",
    "        inter = []\n",
    "    else: \n",
    "        inter.append(working[idx])\n",
    "\n",
    "len(working_chuncked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All accounted for!\n"
     ]
    }
   ],
   "source": [
    "#verify all repos accounted for\n",
    "repos = [\"click\", \"cpython\", \"django\",\"matplotlib\",\"numpy\",\"odoo\",\"pandas\",\"pytest\",\n",
    "         \"pytest-cov\", \"pyyaml\",\"requests\",\"scipy\",\"six\",\"sphinx\"]\n",
    "accounted_for = []\n",
    "for chunk in range(len(working_chuncked)):\n",
    "    for lib in working_chuncked[chunk]:\n",
    "        try: \n",
    "            if len(lib[\"repo_name\"]) > 0:\n",
    "                if lib[\"repo_name\"][0] in repos and lib[\"repo_name\"][0] not in accounted_for:\n",
    "                    accounted_for.append(lib[\"repo_name\"][0])\n",
    "        except: \n",
    "            pass\n",
    "                \n",
    "if len(accounted_for) == len(repos):\n",
    "    print(\"All accounted for!\")\n",
    "else: \n",
    "    print(\"Something went wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up datastructure for analysis\n",
    "'''\n",
    "Main Dict- nodes: \n",
    "    Key: log_in\n",
    "    Value: Dict\n",
    "Sub Dict- relationships: \n",
    "    Key: \"contributor\"\n",
    "    value: list of contributed repos\n",
    "    Key: \"member\" \n",
    "    value: list of memberships repos\n",
    "    Key: Commits\n",
    "    value: # of commits\n",
    "    Key: \"sucessful_requests\" (medium link)\n",
    "    value: List of successful pulls repos\n",
    "    Key: \"failed_requests\" (weak link)\n",
    "    value: List of failed pull repos\n",
    "    \n",
    "'''\n",
    "pax_net = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to extract all nodes\n",
    "def get_actors(working_file):\n",
    "    nodes = []\n",
    "    #Takes a list of the 4 working files and extracts all unique users\n",
    "    #returns list of unique author names\n",
    "    print(working_file[0]['repo_name'][0])\n",
    "    name_list = [\"author_login\", \"login\", \"pull_request_author\", \"actor_login\"]\n",
    "    uniques = []\n",
    "    #use index of list based on changing names\n",
    "    #TODO: Could be made more elegent a lot of duplication\n",
    "    for idx in range(len(working_file)): \n",
    "            uniques += list(working_file[idx][name_list[idx]].unique())\n",
    "            \n",
    "    #print(uniques)\n",
    "    for u in uniques:\n",
    "        if u not in nodes: \n",
    "            nodes.append(u)\n",
    "    \n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create pax net Dictionary as outline above\n",
    "def create_pax_data2(nodes, working_file):\n",
    "    #Takes the unique list of nodes (users) and takes the dataframes csv and creates the above dictionary\n",
    "    for pax in nodes: \n",
    "         \n",
    "            #TODO: add skip clause if in dict\n",
    "            #for pax in db:\n",
    "        if pax in pax_net.keys():\n",
    "            pass\n",
    "        else: \n",
    "            pax_net[pax]={\"contributor\":[],\n",
    "                          \"failed_contrib\":[],\n",
    "                          \"member\":[], \n",
    "                          \"commits\": {},\n",
    "                          \"successful_pulls\":{},\n",
    "                          \"failed_pulls\":{}}\n",
    "    #Add memberships (working_file[1])\n",
    "    for idx, row in working_file[1].iterrows():\n",
    "        if row[\"repo_name\"] not in pax_net[row[\"login\"]][\"member\"]:\n",
    "            pax_net[row[\"login\"]][\"member\"].append(row[\"repo_name\"])\n",
    "    #Add successful commits (working_file[0])\n",
    "    for idx, row in working_file[0].iterrows(): \n",
    "        #point to specific dictionary of interest\n",
    "        author = pax_net[row[\"author_login\"]][\"commits\"]\n",
    "        #print(author)\n",
    "        if row[\"repo_name\"] not in repos:\n",
    "            pass\n",
    "        else: \n",
    "            if row[\"repo_name\"] in author.keys():\n",
    "                author[row[\"repo_name\"]] += 1\n",
    "            else: \n",
    "                author[row[\"repo_name\"]] = 1\n",
    "                pax_net[row[\"author_login\"]][\"contributor\"].append(row[\"repo_name\"])\n",
    "    #add pull requests\n",
    "    for idx, row in working_file[2].iterrows():\n",
    "        #Get pull id and author\n",
    "        author = row[\"pull_request_author\"]\n",
    "        pull_id = row[\"pull_request_id\"]\n",
    "        repo = row[\"repo_name\"]\n",
    "        #determine if pulled request failed or succeeded\n",
    "        pull_history = working_file[3].loc[working_file[3][\"pull_request_id\"]==pull_id]\n",
    "        #pull = working_file[2].loc[working_file[2][\"pull_request_id\"]==pull_id]\n",
    "        #Turn actions into set\n",
    "        pull_actions = set(pull_history['action'])\n",
    "        success_merge = {'opened', \"merged\", \"closed\"}\n",
    "        if success_merge.issubset(pull_actions): \n",
    "            if repo not in pax_net[author][\"contributor\"]:\n",
    "                pax_net[author][\"contributor\"].append(repo)\n",
    "            if repo not in pax_net[author][\"successful_pulls\"].keys():\n",
    "                pax_net[author][\"successful_pulls\"][repo] = 1\n",
    "            else: \n",
    "                pax_net[author][\"successful_pulls\"][repo] += 1\n",
    "        elif \"merged\" not in pull_actions and \"closed\" in pull_actions: \n",
    "            if repo not in pax_net[author][\"failed_contrib\"] and repo not in pax_net[author][\"contributor\"]:\n",
    "                pax_net[author][\"failed_contrib\"].append(repo)\n",
    "            if repo not in pax_net[author][\"failed_pulls\"].keys():\n",
    "                pax_net[author][\"failed_pulls\"][repo] = 1\n",
    "            else: \n",
    "                pax_net[author][\"failed_pulls\"][repo] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "click\n",
      "cpython\n",
      "django\n",
      "matplotlib\n",
      "numpy\n",
      "odoo\n",
      "pandas\n",
      "pytest\n",
      "pytest-cov\n",
      "pyyaml\n",
      "requests\n",
      "scipy\n",
      "setuptools\n",
      "six\n",
      "sphinx\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "337"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run the previous two functions\n",
    "#Get full dictionary\n",
    "for working_file in working_chuncked[0:16]: \n",
    "    nodes = get_actors(working_file)    \n",
    "    create_pax_data2(nodes, working_file)\n",
    "len(nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save to a serialized object\n",
    "import pickle\n",
    "\n",
    "fileObj = open('pax_net3.pkl', 'wb')\n",
    "pickle.dump(pax_net,fileObj)\n",
    "fileObj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
